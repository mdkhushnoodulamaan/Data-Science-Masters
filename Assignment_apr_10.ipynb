{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c0dd73-2d99-4ff3-9aa8-f6218e624d1a",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169d0b0-1801-4809-9b23-d03419cd820d",
   "metadata": {},
   "source": [
    "### Ans1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1198f-5311-402b-80fc-7689d14f27e8",
   "metadata": {},
   "source": [
    "Let's use conditional probability to solve this problem. \n",
    "\n",
    "Let's define the following events:\n",
    "- A: An employee is a smoker.\n",
    "- B: An employee uses the company's health insurance plan.\n",
    "\n",
    "We are given:\n",
    "- P(B) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "- P(A|B) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find:\n",
    "- P(A|B) = ? (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We can use the formula for conditional probability:\n",
    "\\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\]\n",
    "\n",
    "Given that P(A|B) = 0.40 and P(B) = 0.70, we can rearrange the formula to solve for P(A âˆ© B):\n",
    "\\[ P(A \\cap B) = P(A|B) \\times P(B) \\]\n",
    "\\[ P(A \\cap B) = 0.40 \\times 0.70 \\]\n",
    "\\[ P(A \\cap B) = 0.28 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is 0.28 or 28%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5091c-85eb-4c69-862e-7d34e502f06b",
   "metadata": {},
   "source": [
    "### Ans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f47bd5-cc21-431e-b032-f694cb45c235",
   "metadata": {},
   "source": [
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes algorithm used for classification tasks, particularly in natural language processing and text classification. They are both based on the Bayes' theorem and assume that features are conditionally independent given the class. However, they differ in their assumptions about the distribution of features and how they handle feature presence/absence.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Feature Assumption:** Bernoulli Naive Bayes assumes that each feature is binary (0 or 1), representing the presence or absence of a particular term in the input data.\n",
    "   - **Use Case:** It is commonly used for text classification tasks where the presence or absence of certain words in a document is important. For example, sentiment analysis where the presence of specific words indicates positive or negative sentiment.\n",
    "   - **Example:** If we're classifying emails as spam or not spam, each feature (word) is treated as binary (present or absent) in the email.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Feature Assumption:** Multinomial Naive Bayes assumes that features represent counts or frequencies of events. It's often used with data where each feature is a non-negative integer count, such as word frequencies in a document.\n",
    "   - **Use Case:** It's widely used in text classification, such as document categorization or topic classification, where the frequency of words matters.\n",
    "   - **Example:** If we're classifying news articles into different categories, each feature could represent the frequency of a word in the article.\n",
    "\n",
    "In both cases, the Naive Bayes assumption of feature independence (given the class) simplifies calculations, but it might not always hold in practice. Despite this simplification, Naive Bayes models often perform surprisingly well, especially on text data, due to their simplicity and efficiency.\n",
    "\n",
    "It's important to choose the appropriate variant based on the nature of your data and the problem you're trying to solve. If your features are binary (presence/absence) like in text sentiment analysis, Bernoulli Naive Bayes might be more suitable. If your features are counts or frequencies, as often seen in text classification tasks involving word frequencies, Multinomial Naive Bayes could be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67ed99-0339-4109-ad2c-8729a18a64ca",
   "metadata": {},
   "source": [
    "### Ans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009bcf5-b172-44f9-9df3-94816a15c594",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes is specifically designed for binary data, where each feature is either present (1) or absent (0). When it comes to missing values in the context of Bernoulli Naive Bayes, there are a few different approaches you can consider:\n",
    "\n",
    "1. **Ignore Instances with Missing Values:**\n",
    "   One simple approach is to ignore instances (data points) that have missing values in any of the features. This means that if a feature's value is missing (undefined), you exclude that instance from your analysis. However, this approach can result in a loss of valuable data, especially if a significant number of instances have missing values.\n",
    "\n",
    "2. **Impute Missing Values:**\n",
    "   If you want to retain instances with missing values, you might consider imputing the missing values with either 0 or 1 based on certain assumptions. For example, if the missing value represents a feature that you believe should have been present, you might impute it as 1. If you believe it should have been absent, you might impute it as 0. This approach introduces some subjectivity and assumptions.\n",
    "\n",
    "3. **Treat Missing Values as a Separate Category:**\n",
    "   You could treat missing values as a separate category or state for each feature. In this case, you would include a separate category (e.g., \"missing\") in addition to the usual 0 and 1 values for each feature. This allows the model to learn how missing values relate to the target variable independently.\n",
    "\n",
    "4. **Use Advanced Imputation Techniques:**\n",
    "   Depending on the nature of your data, you might choose to use more advanced imputation techniques, such as mean imputation, mode imputation, or even more sophisticated techniques like k-nearest neighbors imputation. These techniques could be applied before using Bernoulli Naive Bayes, but they might introduce complexities and could affect the naive assumption of feature independence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d1ebb-2a3c-4e02-85da-1f5b2cf64e15",
   "metadata": {},
   "source": [
    "### Ans4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43eeb5-0d99-42cb-897c-95eaedb80a97",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can indeed be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes the features in your data follow a Gaussian (normal) distribution. It's well-suited for continuous data, and it's commonly used in cases where the features are real-valued and assumed to be normally distributed within each class.\n",
    "\n",
    "For multi-class classification using Gaussian Naive Bayes, the algorithm is extended to handle more than two classes. Each class's distribution is assumed to follow a Gaussian distribution, and the algorithm calculates the likelihood of an instance belonging to each class based on the Gaussian distribution parameters (mean and variance) of each class's features. The class with the highest likelihood is predicted as the output class for the given instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337eb35-e40f-4c5e-b90a-da634e89bcb1",
   "metadata": {},
   "source": [
    "### Ans5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ad4feb-2735-424b-b788-756b3e2d85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ba58bf-7459-4956-bca4-402df66f7363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...  0.41  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.00   \n",
       "\n",
       "    0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spambase.data')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cceb2ee-010f-40ad-a4e8-13156603a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.iloc[:,:-1].values\n",
    "y= df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34aa6e25-ddad-4587-b620-60d83fb610f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ce318-4882-4245-91ab-e61ed7ad7247",
   "metadata": {},
   "source": [
    "## Bernouli NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f89b02b-50e5-4d07-b1ab-0b29d9dff4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(bernoulli_nb, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14705a0a-8d16-49f8-a6ba-0dc745707211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the fold1: 0.8847826086956522\n",
      "Accuracy of the fold2: 0.9152173913043479\n",
      "Accuracy of the fold3: 0.9021739130434783\n",
      "Accuracy of the fold4: 0.908695652173913\n",
      "Accuracy of the fold5: 0.8913043478260869\n",
      "Accuracy of the fold6: 0.9282608695652174\n",
      "Accuracy of the fold7: 0.9260869565217391\n",
      "Accuracy of the fold8: 0.8913043478260869\n",
      "Accuracy of the fold9: 0.808695652173913\n",
      "Accuracy of the fold10: 0.782608695652174\n",
      "Average Accuracy across all folds: 0.8839130434782609\n",
      "----------------------------------------------------------------------\n",
      "Precision of fold 1: 0.8973111202227066\n",
      "Precision of fold 2: 0.9158117323296708\n",
      "Precision of fold 3: 0.9153763440860215\n",
      "Precision of fold 4: 0.9114535182331793\n",
      "Precision of fold 5: 0.892552645095018\n",
      "Precision of fold 6: 0.9259502749223045\n",
      "Precision of fold 7: 0.9316890789283427\n",
      "Precision of fold 8: 0.8989952406134321\n",
      "Precision of fold 9: 0.7994194484760523\n",
      "Precision of fold 10: 0.7721867321867322\n",
      "Avarage Precision across all folds: 0.886074613509346\n",
      "-------------------------------------------------------------------------------\n",
      "Recall of fold 1: 0.8638825203573405\n",
      "Recall of fold 2: 0.9061388252035734\n",
      "Recall of fold 3: 0.8824828214420087\n",
      "Recall of fold 4: 0.8965920117230044\n",
      "Recall of fold 5: 0.8783738291847363\n",
      "Recall of fold 6: 0.9233945226638152\n",
      "Recall of fold 7: 0.9138398780173864\n",
      "Recall of fold 8: 0.8735222479653062\n",
      "Recall of fold 9: 0.8063922057862531\n",
      "Recall of fold 10: 0.774213350759421\n",
      "Avarage recall across all folds: 0.8718832213102845\n",
      "-------------------------------------------------------------------------------\n",
      "F1 score of fold 1: 0.8398791540785498\n",
      "F1 score of fold 2: 0.8895184135977338\n",
      "F1 score of fold 3: 0.8640483383685801\n",
      "F1 score of fold 4: 0.8786127167630058\n",
      "F1 score of fold 5: 0.8554913294797687\n",
      "F1 score of fold 6: 0.9080779944289694\n",
      "F1 score of fold 7: 0.9011627906976744\n",
      "F1 score of fold 8: 0.851190476190476\n",
      "F1 score of fold 9: 0.7659574468085106\n",
      "F1 score of fold 10: 0.726775956284153\n",
      "Avarage f1 score across all folds: 0.8480714616697421\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of the fold{i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Avarage Precision across all folds: {np.mean(scores['test_precision_macro'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")\n",
    "print(f\"Avarage recall across all folds: {np.mean(scores['test_recall_macro'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 score of fold {i+1}: {scores['test_f1'][i]}\")\n",
    "print(f\"Avarage f1 score across all folds: {np.mean(scores['test_f1'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431c3b8-14ec-40fe-b3c5-bb78dfc35036",
   "metadata": {},
   "source": [
    "## Gaussin NB Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f372a79f-6cb1-4b94-b0ac-8c2780221ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(gaussian_nb, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf358636-0022-4544-9426-dcbbffd8fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the fold1: 0.8434782608695652\n",
      "Accuracy of the fold2: 0.8630434782608696\n",
      "Accuracy of the fold3: 0.8782608695652174\n",
      "Accuracy of the fold4: 0.8673913043478261\n",
      "Accuracy of the fold5: 0.8847826086956522\n",
      "Accuracy of the fold6: 0.8282608695652174\n",
      "Accuracy of the fold7: 0.8326086956521739\n",
      "Accuracy of the fold8: 0.8673913043478261\n",
      "Accuracy of the fold9: 0.6347826086956522\n",
      "Accuracy of the fold10: 0.717391304347826\n",
      "Average Accuracy across all folds: 0.8217391304347826\n",
      "----------------------------------------------------------------------\n",
      "Precision of fold 1: 0.8486742424242424\n",
      "Precision of fold 2: 0.8665588162948668\n",
      "Precision of fold 3: 0.8778048734380612\n",
      "Precision of fold 4: 0.8704222154963681\n",
      "Precision of fold 5: 0.8792075070283414\n",
      "Precision of fold 6: 0.8447074142156863\n",
      "Precision of fold 7: 0.8430462568472307\n",
      "Precision of fold 8: 0.8634469696969698\n",
      "Precision of fold 9: 0.7185854544618426\n",
      "Precision of fold 10: 0.7424239475271999\n",
      "Avarage Precision across all folds: 0.8354877697430808\n",
      "-------------------------------------------------------------------------------\n",
      "Recall of fold 1: 0.8638627559490869\n",
      "Recall of fold 2: 0.88289588109732\n",
      "Recall of fold 3: 0.8957603120853879\n",
      "Recall of fold 4: 0.8877700548525713\n",
      "Recall of fold 5: 0.8953147587080932\n",
      "Recall of fold 6: 0.8564823065803283\n",
      "Recall of fold 7: 0.8571555872393513\n",
      "Recall of fold 8: 0.8800075249014832\n",
      "Recall of fold 9: 0.6882512525000495\n",
      "Recall of fold 10: 0.7466484484841284\n",
      "Avarage recall across all folds: 0.84541488823978\n",
      "-------------------------------------------------------------------------------\n",
      "F1 score of fold 1: 0.8293838862559241\n",
      "F1 score of fold 2: 0.8496420047732697\n",
      "F1 score of fold 3: 0.8634146341463415\n",
      "F1 score of fold 4: 0.8537170263788968\n",
      "F1 score of fold 5: 0.8658227848101266\n",
      "F1 score of fold 6: 0.8192219679633868\n",
      "F1 score of fold 7: 0.8205128205128206\n",
      "F1 score of fold 8: 0.8478802992518704\n",
      "F1 score of fold 9: 0.6692913385826772\n",
      "F1 score of fold 10: 0.711111111111111\n",
      "Avarage f1 score across all folds: 0.8129997873786424\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of the fold{i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Avarage Precision across all folds: {np.mean(scores['test_precision_macro'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")\n",
    "print(f\"Avarage recall across all folds: {np.mean(scores['test_recall_macro'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 score of fold {i+1}: {scores['test_f1'][i]}\")\n",
    "print(f\"Avarage f1 score across all folds: {np.mean(scores['test_f1'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d4bca-1583-44e7-93ad-417bdf54ae4c",
   "metadata": {},
   "source": [
    "## Multinomial NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2cb5e7-2471-42fb-a517-f2223db68ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(multinomial_nb, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a5ce9f9-9bc7-472a-b4bf-0d5e41e2144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the fold1: 0.7913043478260869\n",
      "Accuracy of the fold2: 0.7891304347826087\n",
      "Accuracy of the fold3: 0.8108695652173913\n",
      "Accuracy of the fold4: 0.8347826086956521\n",
      "Accuracy of the fold5: 0.8282608695652174\n",
      "Accuracy of the fold6: 0.7782608695652173\n",
      "Accuracy of the fold7: 0.7782608695652173\n",
      "Accuracy of the fold8: 0.8130434782608695\n",
      "Accuracy of the fold9: 0.6934782608695652\n",
      "Accuracy of the fold10: 0.7434782608695653\n",
      "Average Accuracy across all folds: 0.786086956521739\n",
      "----------------------------------------------------------------------\n",
      "Precision of fold 1: 0.7842799770510613\n",
      "Precision of fold 2: 0.784690252464737\n",
      "Precision of fold 3: 0.8042645140247879\n",
      "Precision of fold 4: 0.8276337066538899\n",
      "Precision of fold 5: 0.8230185909980431\n",
      "Precision of fold 6: 0.7678085051392671\n",
      "Precision of fold 7: 0.7676658476658477\n",
      "Precision of fold 8: 0.8081634339303051\n",
      "Precision of fold 9: 0.6963907384987893\n",
      "Precision of fold 10: 0.7314987714987715\n",
      "Avarage Precision across all folds: 0.77954143379255\n",
      "-------------------------------------------------------------------------------\n",
      "Recall of fold 1: 0.7742114001106807\n",
      "Recall of fold 2: 0.7676693809787335\n",
      "Recall of fold 3: 0.7955702093110755\n",
      "Recall of fold 4: 0.8249866333986812\n",
      "Recall of fold 5: 0.8137883918493436\n",
      "Recall of fold 6: 0.7667478563931959\n",
      "Recall of fold 7: 0.769658805124854\n",
      "Recall of fold 8: 0.7954216915186438\n",
      "Recall of fold 9: 0.7055882294698905\n",
      "Recall of fold 10: 0.7332224400483178\n",
      "Avarage recall across all folds: 0.7746865038203417\n",
      "-------------------------------------------------------------------------------\n",
      "F1 score of fold 1: 0.7241379310344828\n",
      "F1 score of fold 2: 0.7138643067846608\n",
      "F1 score of fold 3: 0.7507163323782237\n",
      "F1 score of fold 4: 0.7877094972067039\n",
      "F1 score of fold 5: 0.7736389684813754\n",
      "F1 score of fold 6: 0.7166666666666666\n",
      "F1 score of fold 7: 0.7213114754098361\n",
      "F1 score of fold 8: 0.75\n",
      "F1 score of fold 9: 0.6618705035971223\n",
      "F1 score of fold 10: 0.6775956284153005\n",
      "Avarage f1 score across all folds: 0.7277511309974372\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of the fold{i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Avarage Precision across all folds: {np.mean(scores['test_precision_macro'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")\n",
    "print(f\"Avarage recall across all folds: {np.mean(scores['test_recall_macro'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 score of fold {i+1}: {scores['test_f1'][i]}\")\n",
    "print(f\"Avarage f1 score across all folds: {np.mean(scores['test_f1'])}\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a8d46-f415-40ca-9608-a2f4b33adc05",
   "metadata": {},
   "source": [
    "### Discuss about the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28ce26-6586-4c88-a6cc-560f97e54f42",
   "metadata": {},
   "source": [
    "- Comparing the average f1 score across all the folds we can say that Bernoulli Naive Bayes Classifer has the best performence on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d3d368-e553-4d30-951e-fd0f17232a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##splitting into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6471c4c8-6796-47e9-b776-a9ede0b47f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "245ab6b3-2d9d-40e2-aaab-da3aedcd6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = bernoulli_nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "824a2afe-0926-4b98-a295-058be08db4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       530\n",
      "           1       0.89      0.79      0.84       390\n",
      "\n",
      "    accuracy                           0.87       920\n",
      "   macro avg       0.88      0.86      0.87       920\n",
      "weighted avg       0.87      0.87      0.87       920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3844cc-bb03-4e15-9b6d-c321c734299c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_preds, labels=bernoulli_nb.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=bernoulli_nb.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a185a-6e23-47d7-a0d0-f4e4d0b7201a",
   "metadata": {},
   "source": [
    "## Summary of confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f48d9-3339-4355-8937-df3af983b953",
   "metadata": {},
   "source": [
    "- The confusion matrix indicates that the model has correctly predicted the majority of instances for both classes, with a higher accuracy in predicting class 1. But it has made some incorrect predictions, with a higher number of false negatives for class 0 and false positives for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501de8f-4d3b-4ebb-a212-96b0e9c40746",
   "metadata": {},
   "source": [
    "## Suggestion for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71405b-f68a-4fad-bf9c-6917f238b168",
   "metadata": {},
   "source": [
    "We can improve this data with more impletation like we can\n",
    "\n",
    "- Perform hyperparameter tuning \n",
    "- We can use Random forest Classifier or XGBoost\n",
    "- Implement ensemble techiniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b37d3-8065-4396-b2d2-529fdd8339ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
