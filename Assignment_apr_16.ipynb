{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbbfdf6-9056-40b1-b501-820bd4055325",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877d237-51af-4ab1-a40a-016153fb0937",
   "metadata": {},
   "source": [
    "### Ans1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf68bf7-80ac-43ec-93d6-f5fdc889863e",
   "metadata": {},
   "source": [
    "Boosting is a machine learning technique that combines the predictions from multiple weak learners (usually simple models) to create a single strong learner. The primary goal of boosting is to improve the predictive performance of a model by reducing bias and increasing accuracy. It's a popular ensemble learning method used for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68cd3e-22b7-4749-86b0-61537607bc78",
   "metadata": {},
   "source": [
    "### Ans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4ed74-a0c6-480f-b097-6f169fab89fd",
   "metadata": {},
   "source": [
    "Boosting techniques offer several advantages in machine learning, but they also come with some limitations. Here's an overview of both:\n",
    "\n",
    "Advantages of Boosting Techniques:\n",
    "\n",
    "1) Improved Predictive Performance: Boosting often results in significantly improved predictive accuracy compared to using a single weak learner. It helps reduce both bias and variance in the final model, leading to better generalization to unseen data.\n",
    "\n",
    "2) Handles Complex Relationships: Boosting can capture complex relationships in the data, making it suitable for tasks where the underlying patterns are intricate and non-linear.\n",
    "\n",
    "3) Robust to Overfitting: Boosting algorithms are less prone to overfitting compared to deep learning methods like neural networks. This is due to the ensemble nature of boosting, where multiple weak learners are combined, preventing excessive memorization of the training data.\n",
    "\n",
    "\n",
    "Limitations of Boosting Techniques:\n",
    "\n",
    "1) Sensitivity to Noisy Data: Boosting can be sensitive to noisy or outlier data points. Since it assigns higher weights to misclassified instances, outliers or noisy examples can have a disproportionate influence on the model.\n",
    "\n",
    "2) Computationally Intensive: Some boosting algorithms, especially gradient boosting methods like XGBoost and LightGBM, can be computationally expensive and require significant resources, making them less suitable for real-time or resource-constrained applications.\n",
    "\n",
    "3) Prone to Overfitting with Too Many Weak Learners: While boosting is less prone to overfitting compared to individual decision trees, it can still overfit if too many weak learners are added to the ensemble. Careful hyperparameter tuning is required to avoid this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88ec31-d90b-4765-8cff-0113edd680cf",
   "metadata": {},
   "source": [
    "### Ans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287ff4a-78de-4c6d-8107-d3460e16da8d",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique that works by combining the predictions of multiple weak learners (simple models) to create a single strong learner. The process of boosting can be understood in a step-by-step manner:\n",
    "\n",
    "1. **Initialization**: Boosting starts with an initial dataset and assigns equal weights to all data points in that dataset. These weights determine the importance of each data point in the training process.\n",
    "\n",
    "2. **Sequential Training**: Boosting builds a sequence of weak learners iteratively. Each iteration focuses on improving the areas where the previous weak learners made mistakes. Here's how each iteration works:\n",
    "\n",
    "   a. **Fit a Weak Learner**: In each iteration, a weak learner (e.g., a decision stump or a shallow decision tree) is trained on the current dataset. The weak learner's goal is to make predictions that are better than random guessing.\n",
    "\n",
    "   b. **Weighted Error Calculation**: After training the weak learner, its performance is evaluated on the training dataset. Data points that the weak learner misclassifies or predicts poorly are assigned higher weights for the next iteration, making them more important for the next weak learner to focus on.\n",
    "\n",
    "   c. **Update Weights**: The weights of the data points are updated based on their performance. Misclassified points get higher weights, while correctly classified points get lower weights. This adjustment gives more importance to the previously misclassified data points.\n",
    "\n",
    "   d. **Build the Ensemble**: Each weak learner is assigned a weight based on its performance. Learners that perform well typically have higher weights, while those that perform poorly have lower weights. These weights reflect the contribution of each weak learner to the final prediction.\n",
    "\n",
    "3. **Combine Weak Learners**: To make predictions, the boosting algorithm combines the individual predictions of all weak learners using a weighted voting scheme. Stronger learners have more influence on the final prediction.\n",
    "\n",
    "4. **Repeat Iterations**: Steps 2 and 3 are repeated for a predefined number of iterations or until a stopping criterion is met. Common stopping criteria include achieving a certain level of accuracy or when the algorithm converges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25a670-848d-4464-8c90-12c1726dc212",
   "metadata": {},
   "source": [
    "### Ans4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa085f7c-ee9c-452e-95ce-102bba2ff2d9",
   "metadata": {},
   "source": [
    "There are several different types of boosting algorithms, each with its own variations and characteristics. Some of the most well-known boosting algorithms include:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):** AdaBoost is one of the earliest and most popular boosting algorithms. It assigns higher weights to misclassified data points in each iteration, allowing weak learners to focus on the examples that are difficult to classify. AdaBoost combines the predictions of weak learners through a weighted majority vote.\n",
    "\n",
    "2. **Gradient Boosting Machines (GBM):** Gradient Boosting is a general boosting framework that builds an ensemble of weak learners in a sequential manner. It uses gradient descent optimization to minimize a loss function. Variants of GBM include:\n",
    "\n",
    "   - **XGBoost:** Extreme Gradient Boosting is an optimized and efficient implementation of gradient boosting. It includes regularization techniques, parallel processing, and tree pruning, making it one of the most popular choices for structured/tabular data.\n",
    "   \n",
    "   - **LightGBM:** Light Gradient Boosting Machine is designed for efficiency and can handle large datasets. It uses histogram-based techniques and is known for its speed.\n",
    "   \n",
    "   - **CatBoost:** Categorical Boosting is a boosting algorithm that handles categorical features effectively. It can automatically handle categorical data without the need for extensive preprocessing.\n",
    "\n",
    "3. **Stochastic Gradient Boosting (SGD):** Stochastic Gradient Boosting combines gradient boosting with stochastic gradient descent. It randomly subsamples the training data in each iteration, making it faster and more scalable for large datasets.\n",
    "\n",
    "4. **Histogram-Based Boosting:** Some boosting algorithms, like LightGBM and CatBoost, use histogram-based methods to speed up the training process. Instead of looking at individual data points, they group data into bins and calculate splits based on histograms, reducing computation time.\n",
    "\n",
    "5. **LogitBoost:** LogitBoost is a boosting algorithm specifically designed for binary classification tasks. It optimizes the logistic loss function and updates weights accordingly.\n",
    "\n",
    "6. **BrownBoost:** BrownBoost is an extension of AdaBoost that incorporates the margin distribution of the training data. It aims to increase the margins between the classes.\n",
    "\n",
    "7. **LPBoost (Linear Programming Boosting):** LPBoost is a boosting algorithm that uses linear programming to optimize the weighted combination of weak learners.\n",
    "\n",
    "8. **TotalBoost:** TotalBoost is a boosting algorithm that combines boosting with a penalty for model complexity, helping to prevent overfitting.\n",
    "\n",
    "9. **LPBoost (Linear Programming Boosting):** LPBoost is a boosting algorithm that uses linear programming to optimize the weighted combination of weak learners.\n",
    "\n",
    "10. **TotalBoost:** TotalBoost is a boosting algorithm that combines boosting with a penalty for model complexity, helping to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472dfc5a-9a68-4ce9-bcd1-d9bc31e5f9a7",
   "metadata": {},
   "source": [
    "### Ans5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869483b-6702-4457-ae60-9fd0ae7598f5",
   "metadata": {},
   "source": [
    "Boosting algorithms typically have several parameters that can be tuned to optimize the performance of the model. Here are some common parameters that you may encounter when working with boosting algorithms:\n",
    "\n",
    "1. **Number of Estimators (n_estimators):** This parameter determines the number of weak learners (e.g., decision trees or linear models) to be used in the ensemble. Increasing the number of estimators can improve the model's performance but may also increase the risk of overfitting.\n",
    "\n",
    "2. **Learning Rate (or Shrinkage) (learning_rate):** The learning rate controls the contribution of each weak learner to the ensemble. Lower values make the learning process slower but can lead to better generalization. It is often used in gradient boosting algorithms like XGBoost and LightGBM.\n",
    "\n",
    "3. **Maximum Depth of Weak Learners (max_depth):** In boosting algorithms that use decision trees as weak learners, this parameter controls the maximum depth of the individual trees. It helps prevent overfitting and limits the complexity of the base models.\n",
    "\n",
    "4. **Minimum Samples per Leaf (min_samples_leaf):** This parameter specifies the minimum number of samples required to create a leaf node in a decision tree. It helps control the complexity of the individual trees and can prevent overfitting.\n",
    "\n",
    "5. **Subsampling (subsample):** Subsampling controls the fraction of the training data used in each iteration. It can be used to introduce randomness and reduce overfitting. A value less than 1.0 means that a random subset of the data is used in each iteration.\n",
    "\n",
    "6. **Feature Subsampling (colsample_bytree, colsample_bylevel, colsample_bynode):** These parameters control the fraction of features (columns) considered at each split of a decision tree. Feature subsampling can help reduce overfitting and improve model generalization.\n",
    "\n",
    "7. **Regularization Parameters (e.g., lambda, alpha):** Some boosting algorithms include regularization terms to control the complexity of the model. These parameters penalize large coefficients in linear models or large tree structures in decision trees.\n",
    "\n",
    "8. **Loss Function (loss):** The choice of loss function determines the objective that the boosting algorithm tries to optimize. Common options include \"linear\" for linear regression, \"logistic\" for logistic regression, and various options for classification tasks (e.g., \"exponential\" in AdaBoost).\n",
    "\n",
    "9. **Early Stopping (n_iter_no_change, early_stopping_rounds):** Early stopping allows you to halt the boosting process when the model's performance on a validation set stops improving. It helps prevent overfitting and can save training time.\n",
    "\n",
    "10. **Base Learner Type (base_estimator):** In some boosting algorithms, you can specify the type of weak learner to use, such as decision stumps (a tree with a single split), linear models, or more complex models like decision trees.\n",
    "\n",
    "11. **Class Weights (class_weight):** In classification tasks, you can assign different weights to different classes to handle class imbalance.\n",
    "\n",
    "12. **Scoring Metric (eval_metric):** This parameter determines the evaluation metric used during training. Common metrics include accuracy, mean squared error (MSE), area under the ROC curve (AUC-ROC), and log-loss for classification tasks.\n",
    "\n",
    "13. **Random Seed (random_state):** Setting a random seed ensures reproducibility of results, as it initializes the random number generator to the same state for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabd0fa-7252-4403-8cd9-9e6e72b102ee",
   "metadata": {},
   "source": [
    "### Ans6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2af312-00db-4c33-854f-37dd26fdc90f",
   "metadata": {},
   "source": [
    "Boosting algorithms combine weak learners to create a strong learner through a weighted or adaptive ensemble process. The key idea is to assign weights to the individual weak learners and their predictions, allowing the strong learner to give more importance to the better-performing weak learners. Here's a general overview of how boosting algorithms combine weak learners:\n",
    "\n",
    "1. **Initialization**: Boosting starts with an initial model, often a simple weak learner like a decision stump (a tree with a single split) or a linear model. The initial model's predictions are used as the starting point for the ensemble.\n",
    "\n",
    "2. **Sequential Training**: Boosting algorithms build the ensemble of weak learners sequentially, with each new learner focusing on the areas where the previous ones made mistakes. This process involves several steps:\n",
    "\n",
    "   a. **Training a Weak Learner**: In each iteration, a new weak learner is trained on the dataset. This learner aims to capture the patterns or relationships in the data that the current ensemble finds difficult to model accurately.\n",
    "\n",
    "   b. **Weighted Voting**: After training the weak learner, its predictions are combined with the predictions of the existing ensemble. The contributions of the weak learner are weighted based on its performance. Better-performing weak learners receive higher weights, while poorer-performing ones receive lower weights.\n",
    "\n",
    "   c. **Updating Weights**: The algorithm updates the weights of the training instances (data points) based on the performance of the ensemble. Instances that were misclassified or had incorrect predictions in the previous iteration are assigned higher weights, making them more important in the next iteration.\n",
    "\n",
    "   d. **Adding to the Ensemble**: The newly trained weak learner, along with its associated weight, is added to the ensemble. This step expands the ensemble's capacity to capture complex patterns in the data.\n",
    "\n",
    "3. **Final Prediction**: To make a final prediction, boosting algorithms combine the predictions of all weak learners in the ensemble. Typically, this is done through a weighted majority vote or weighted averaging. Stronger learners, with higher weights, have a more significant influence on the final prediction.\n",
    "\n",
    "4. **Stopping Criterion**: Boosting continues to add weak learners until a predefined stopping criterion is met, such as reaching a certain number of iterations or achieving a desired level of accuracy. This prevents overfitting and ensures the model generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbd028-f12d-4361-a630-21dbb3f8a313",
   "metadata": {},
   "source": [
    "### Ans7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79b045-8534-4b72-93f4-8116d305e64e",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is one of the earliest and most well-known boosting algorithms in machine learning. It was introduced by Yoav Freund and Robert Schapire in 1996. AdaBoost is primarily used for binary classification problems but can be extended to multiclass classification as well. The key idea behind AdaBoost is to sequentially train a series of weak learners (typically decision stumps) and combine their predictions to create a strong classifier.\n",
    "\n",
    "Here's how AdaBoost works:\n",
    "\n",
    "1. **Initialization**: AdaBoost starts with an initial uniform weight distribution over the training data. Each data point is assigned an equal weight, so all data points are equally important in the first iteration.\n",
    "\n",
    "2. **Sequential Training**:\n",
    "   \n",
    "   a. **Train a Weak Learner**: In each iteration, AdaBoost trains a weak learner (often a decision stump) on the training data. The goal of the weak learner is to perform slightly better than random guessing.\n",
    "\n",
    "   b. **Weighted Error Calculation**: After training the weak learner, its performance on the training data is evaluated. The error rate (misclassification rate) of the weak learner is computed, and it measures how well the weak learner's predictions match the true labels.\n",
    "\n",
    "   c. **Compute Weak Learner Weight**: AdaBoost assigns a weight to the weak learner based on its error rate. Weak learners with lower error rates are given higher weights, indicating that they are more accurate and should have a larger say in the final prediction.\n",
    "\n",
    "   d. **Update Weights**: AdaBoost updates the weights of the training data points. It increases the weights of the data points that the current weak learner misclassified. This emphasizes the importance of the misclassified points for the next iteration.\n",
    "\n",
    "3. **Combine Weak Learners**:\n",
    "   \n",
    "   a. AdaBoost combines the weak learners by giving each of them a weight based on their performance. Better-performing weak learners have higher weights in the final ensemble.\n",
    "\n",
    "   b. To make predictions, AdaBoost assigns a weight to each weak learner's prediction, and these weighted predictions are summed. The final prediction is determined by a weighted majority vote.\n",
    "\n",
    "4. **Stopping Criterion**:\n",
    "   \n",
    "   a. AdaBoost continues the sequential training process for a predefined number of iterations (controlled by the user-defined parameter `n_estimators`) or until it reaches a desired level of accuracy.\n",
    "\n",
    "   b. Alternatively, AdaBoost can use an early stopping criterion, such as achieving a specific accuracy on the training data or observing no further improvement.\n",
    "\n",
    "5. **Final Model**:\n",
    "   \n",
    "   a. The resulting AdaBoost model is a weighted combination of the individual weak learners. Weak learners that perform well on the training data have a higher influence on the final prediction.\n",
    "\n",
    "   b. The final model is capable of making accurate predictions on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0d0ad-a4d0-4ba7-82e9-3ec29add10e7",
   "metadata": {},
   "source": [
    "### Ans8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b5f1f-60f6-4c20-ae68-1bf1fd71ecee",
   "metadata": {},
   "source": [
    "The AdaBoost (Adaptive Boosting) algorithm uses the exponential loss function (also known as the exponential loss or AdaBoost loss) as its default loss function. The exponential loss is a commonly used loss function in AdaBoost for binary classification tasks. It is defined as follows:\n",
    "\n",
    "Exponential Loss (AdaBoost Loss):\n",
    "\\[L(y, f(x)) = e^{-yf(x)}\\]\n",
    "\n",
    "Where:\n",
    "- \\(L(y, f(x))\\) is the exponential loss for a data point with true label \\(y\\) and predicted score \\(f(x)\\).\n",
    "- \\(y\\) is the true label for the data point, which is either +1 or -1 for binary classification, representing the positive and negative classes, respectively.\n",
    "- \\(f(x)\\) is the raw score or prediction made by the AdaBoost model for the data point.\n",
    "\n",
    "In the context of AdaBoost, the goal is to minimize this exponential loss during the training process. The algorithm focuses on reducing the exponential loss for data points that are misclassified or have high confidence scores (positive or negative) assigned by the weak learners in each iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105bc866-618b-4684-9a14-2a58fcae02c7",
   "metadata": {},
   "source": [
    "### Ans9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf522d-5e0b-44e7-bea3-47982773839d",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm updates the weights of misclassified samples in each iteration to emphasize the importance of these samples for subsequent iterations. The purpose of this weight update is to focus on the data points that are difficult to classify correctly, allowing the algorithm to learn from its mistakes and improve its performance. Here's how AdaBoost updates the weights of misclassified samples:\n",
    "\n",
    "1. **Initialization**: In the first iteration, AdaBoost starts with an initial weight distribution over the training samples. All samples are assigned equal weights, so \\(w_i = 1/N\\) for \\(i = 1, 2, \\ldots, N\\), where \\(N\\) is the number of training samples.\n",
    "\n",
    "2. **Training Weak Learner**: AdaBoost trains a weak learner (e.g., a decision stump) on the training data using the current weight distribution.\n",
    "\n",
    "3. **Weighted Error Calculation**: After training the weak learner, it makes predictions on the training data. The predictions are compared to the true labels, and the weighted error (often denoted as \\(\\epsilon_t\\)) of the weak learner for the current iteration is computed. The weighted error measures how well the weak learner's predictions match the true labels while considering the current weights of the samples.\n",
    "\n",
    "   \\[ \\epsilon_t = \\sum_{i=1}^{N} w_i \\cdot \\mathbb{I}(y_i \\neq h_t(x_i)) \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(w_i\\) is the weight of the \\(i\\)-th training sample.\n",
    "   - \\(y_i\\) is the true label of the \\(i\\)-th sample.\n",
    "   - \\(h_t(x_i)\\) is the prediction made by the weak learner for the \\(i\\)-th sample in the current iteration.\n",
    "   - \\(\\mathbb{I}(\\cdot)\\) is the indicator function, which is 1 if the condition inside the parentheses is true and 0 otherwise.\n",
    "\n",
    "4. **Compute Weak Learner Weight**: AdaBoost calculates the weight (\\(\\alpha_t\\)) of the current weak learner based on its weighted error (\\(\\epsilon_t\\)) for the current iteration. The formula for \\(\\alpha_t\\) is:\n",
    "\n",
    "   \\[ \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right) \\]\n",
    "\n",
    "   The term \\(\\ln\\) represents the natural logarithm. The factor of \\(\\frac{1}{2}\\) ensures that \\(\\alpha_t\\) is positive regardless of whether the weak learner's error is above or below 50%. This weight (\\(\\alpha_t\\)) reflects the accuracy of the current weak learner's predictions, with better-performing learners receiving higher weights.\n",
    "\n",
    "5. **Update Sample Weights**: The weight of each training sample (\\(w_i\\)) is updated based on the weak learner's performance and the computed \\(\\alpha_t\\). The update formula for \\(w_i\\) is as follows:\n",
    "\n",
    "   \\[ w_i^{(t+1)} = w_i^{(t)} \\cdot \\exp\\left(-\\alpha_t \\cdot y_i \\cdot h_t(x_i)\\right) \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(w_i^{(t)}\\) is the weight of the \\(i\\)-th training sample in the current iteration.\n",
    "   - \\(y_i\\) is the true label of the \\(i\\)-th sample.\n",
    "   - \\(h_t(x_i)\\) is the prediction made by the current weak learner for the \\(i\\)-th sample in the current iteration.\n",
    "   - \\(\\alpha_t\\) is the weight assigned to the current weak learner.\n",
    "\n",
    "6. **Normalization of Sample Weights**: After updating the sample weights, AdaBoost normalizes them so that they sum to 1. This normalization ensures that the sample weights remain a valid probability distribution.\n",
    "\n",
    "7. **Repeat Iterations or Stop**: Steps 2 to 6 are repeated for a predefined number of iterations or until a stopping criterion is met (e.g., achieving a desired level of accuracy or a maximum number of iterations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fccc57-c182-4b5a-b62f-6f647c86854a",
   "metadata": {},
   "source": [
    "### Ans10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44064f-de80-43e5-bf75-f0f276179d9b",
   "metadata": {},
   "source": [
    "Increasing the number of estimators (also known as weak learners or base models) in the AdaBoost algorithm can have both positive and negative effects on the model's performance and training process. The primary effect of increasing the number of estimators is to make the AdaBoost ensemble more complex. Here's a closer look at how the number of estimators affects AdaBoost:\n",
    "\n",
    "**Positive Effects:**\n",
    "\n",
    "1. **Improved Training Accuracy:** Increasing the number of estimators often leads to improved training accuracy. With more weak learners, the AdaBoost algorithm has more opportunities to correct mistakes made by the previous learners. It can gradually reduce the training error by focusing on difficult-to-classify data points.\n",
    "\n",
    "2. **Better Generalization:** In many cases, increasing the number of estimators can also lead to better generalization to unseen data. By iteratively adding well-tuned weak learners, AdaBoost can capture more complex patterns in the data, resulting in a stronger and more robust model.\n",
    "\n",
    "3. **Reduced Bias:** As the number of estimators grows, the AdaBoost model becomes less biased because it has the capacity to represent more intricate decision boundaries. This can make it more suitable for tasks with complex relationships between features and labels.\n",
    "\n",
    "**Negative Effects:**\n",
    "\n",
    "1. **Slower Training:** Increasing the number of estimators requires training and evaluating more weak learners in each iteration. Consequently, training time can increase significantly as you add more estimators, especially if the base models are computationally expensive. AdaBoost can become slower and less practical for large datasets or resource-constrained environments.\n",
    "\n",
    "2. **Risk of Overfitting:** While AdaBoost is less prone to overfitting compared to individual decision trees, increasing the number of estimators can still lead to overfitting if the model becomes excessively complex. Careful monitoring of model performance on a validation set and potential early stopping may be necessary to prevent overfitting.\n",
    "\n",
    "3. **Diminishing Returns:** After a certain point, adding more estimators may result in diminishing returns in terms of performance improvement. The gains in accuracy and generalization may become marginal, and training time may become prohibitively long.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b47fe-e1db-49c4-b31d-0a1349722ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
