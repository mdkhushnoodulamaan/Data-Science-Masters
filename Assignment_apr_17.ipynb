{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dec8a10-92b3-44cf-90ab-0b27a4184b69",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d2ba-a55b-4e76-8536-42997c21232d",
   "metadata": {},
   "source": [
    "### Ans1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b6b6b-a725-4417-adf1-388f0b9ddaf9",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, which involve predicting a continuous numeric target variable based on input features. It is a type of ensemble learning method that combines the predictions of multiple individual regression models, typically decision trees, to create a more accurate and robust predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a631b-ce6f-4a0c-b1f5-144abf5272b6",
   "metadata": {},
   "source": [
    "### Ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba96cbcf-39a0-4c2c-a54e-3720f8a511a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error : 0.140\n",
      "R-squared: 0.679\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.residuals = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.residuals = y - np.mean(y)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, self.residuals)\n",
    "            residuals_pred = tree.predict(X)\n",
    "            self.residuals -= self.learning_rate * residuals_pred\n",
    "            self.trees.append(tree)\n",
    "    def predict(self, X):\n",
    "        residuals_pred = np.sum(tree.predict(X) for tree in self.trees)\n",
    "        return np.mean(y) + self.learning_rate * residuals_pred\n",
    "    \n",
    "    \n",
    "X = np.random.rand(100,5)\n",
    "y = np.sum(X, axis=1) + np.random.normal(scale=0.1, size= 100)\n",
    "\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gbr.fit(X_train,y_train)\n",
    "\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "mse = np.mean((y_pred-y_test)**2)\n",
    "r2 = 1 - mse/ np.var(y_test)\n",
    "\n",
    "print(\"Mean squared error : {:.3f}\".format(mse))\n",
    "print(\"R-squared: {:.3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc620e-a8ba-4c6d-a9de-7d7d1ad9819f",
   "metadata": {},
   "source": [
    "### Ans3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094ff855-4738-4ce9-b886-334018e28fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}\n",
      "Mean Squared Error (MSE): 6.338156869469146\n",
      "R-squared (R^2): 0.9135711182987436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R-squared (R^2)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R^2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37aef4-2abd-4d13-bcc3-f1481c77cf72",
   "metadata": {},
   "source": [
    "### Ans4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab665c7-262b-43e7-b298-1837bc96c3d1",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a \"weak learner\" refers to a base learning algorithm that performs slightly better than random guessing on a given problem. Weak learners are typically decision trees with a shallow depth, often referred to as \"stumps\" or \"shallow trees.\" These trees are simple and have limited predictive power when used individually.\n",
    "\n",
    "Weak learners are a fundamental component of Gradient Boosting algorithms, such as AdaBoost and Gradient Boosting Machines (GBM). The idea behind boosting is to combine the predictions of multiple weak learners to create a strong ensemble model that can make highly accurate predictions.\n",
    "\n",
    "The main characteristics of weak learners in Gradient Boosting are:\n",
    "\n",
    "1. **Low Complexity**: Weak learners are intentionally kept simple and have low model complexity. They are usually constrained to have a limited number of nodes or a shallow depth, typically just one or two splits in the case of decision trees.\n",
    "\n",
    "2. **Slight Advantage over Random Guessing**: While weak learners perform better than random guessing, they are not individually powerful enough to solve the entire problem on their own. Their accuracy is typically only slightly better than chance.\n",
    "\n",
    "3. **Sequential Training**: In Gradient Boosting, weak learners are trained sequentially, and each new weak learner is trained to correct the errors or residuals of the ensemble formed by the previously trained learners. This iterative process continues until a predefined stopping criterion is met or until the ensemble achieves satisfactory performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6297f31-54b4-4989-bdee-0c15758d7a2b",
   "metadata": {},
   "source": [
    "### Ans5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762198fa-a336-453f-b1b5-d21b5d0b052a",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. **Combining Weak Predictors**: Gradient Boosting aims to combine the predictions of multiple \"weak learners\" (typically simple decision trees) to create a powerful ensemble model. Each weak learner is focused on correcting the errors made by the ensemble of previously trained learners.\n",
    "\n",
    "2. **Sequential Correction**: The training process is sequential. It starts with the first weak learner attempting to fit the data. Subsequent learners are trained to correct the errors (residuals) of the ensemble formed by the previous learners. This sequential correction is a key feature of Gradient Boosting.\n",
    "\n",
    "3. **Gradient Descent Optimization**: The name \"Gradient Boosting\" comes from the use of gradient descent optimization to minimize a loss function. In each iteration, the algorithm calculates the gradient (derivative) of the loss function with respect to the ensemble's predictions and then fits a weak learner to this gradient. This step helps to reduce the errors made by the current ensemble.\n",
    "\n",
    "4. **Weighted Combination**: The predictions of each weak learner are combined in a weighted manner to form the final ensemble prediction. Weak learners that perform well on correcting errors are given more weight in the combination, while those that perform poorly are given less weight.\n",
    "\n",
    "5. **Regularization**: To prevent overfitting, Gradient Boosting typically employs regularization techniques such as tree depth constraints (shallow trees) and learning rate adjustment. These techniques help control the complexity of the ensemble.\n",
    "\n",
    "6. **Ensemble's Complexity**: As the boosting process continues, the ensemble becomes more and more complex, capturing both simple and intricate patterns in the data. The sequential nature of training ensures that each new weak learner focuses on the areas where the previous learners have made errors.\n",
    "\n",
    "7. **High Predictive Accuracy**: The iterative nature of Gradient Boosting allows it to incrementally improve the model's accuracy. By continuously reducing the errors, the algorithm can ultimately produce highly accurate predictions, often outperforming other machine learning algorithms.\n",
    "\n",
    "8. **Versatility**: Gradient Boosting is a versatile algorithm that can be used for both regression and classification tasks. It can handle a wide range of data types and has been successful in various domains, from finance to natural language processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a4516-eae3-4968-9031-d042debfbc7a",
   "metadata": {},
   "source": [
    "### Ans6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9da56-f76e-459a-8c08-954c7839f7d0",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative and sequential manner. Here's a step-by-step explanation of how it constructs this ensemble:\n",
    "\n",
    "1. **Initialization**: Gradient Boosting starts by initializing the ensemble with a simple model, often a constant value or a simple linear regression model. This initial prediction serves as the starting point.\n",
    "\n",
    "2. **Calculate Residuals**: In each iteration, the algorithm calculates the residuals, which represent the errors or discrepancies between the current ensemble's predictions and the true target values. The residuals indicate where the current model is making mistakes.\n",
    "\n",
    "3. **Fit a Weak Learner to Residuals**: The algorithm then fits a weak learner (usually a decision tree with limited depth) to the residuals. This weak learner's purpose is to capture and correct the errors made by the current ensemble. It finds patterns or relationships in the data that are not yet captured by the ensemble.\n",
    "\n",
    "4. **Update Ensemble Predictions**: The predictions from the newly trained weak learner are combined with the predictions of the current ensemble. This combination involves adding the predictions from the weak learner to the predictions from the previous ensemble, with a scaling factor (learning rate) applied. The scaling factor controls the contribution of each weak learner to the ensemble.\n",
    "\n",
    "5. **Repeat**: Steps 2 to 4 are repeated for a specified number of iterations (controlled by the hyperparameter `n_estimators`) or until some predefined stopping criterion is met. The algorithm continues to focus on correcting the errors and improving the ensemble's predictions.\n",
    "\n",
    "6. **Final Ensemble**: The final ensemble is formed by combining the predictions of all the weak learners trained during the iterations. Each weak learner's contribution is weighted based on its performance in reducing the residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ae5a5-1393-4c6a-98e4-97bd23d000bc",
   "metadata": {},
   "source": [
    "### Ans7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b99944-c13f-4dd1-bfc7-8266d27ab659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
