{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1470cd70-02da-46f8-b5de-bc4a6e0bb77d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd2e5a-bf60-45f8-a118-a0a10354ffcc",
   "metadata": {},
   "source": [
    "### Ans1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de49951-8cc6-4e1c-aee9-cd009e5b0f45",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data for one or more variables in a given observation or record. The missing value may occur due to various reasons, such as incomplete data collection, data entry errors, or data corruption during transmission or storage.\n",
    "\n",
    "Some algorithms that are not affected by missing values include decision trees, random forests, and support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b371764-9449-4776-900a-75dfed5b376d",
   "metadata": {},
   "source": [
    "### Ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de2e07a-e318-4ac5-bfe2-334f156239e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"A\":[1,2,3,np.nan,4],\n",
    "                  \"B\":[3,4,5,np.nan,np.nan],\n",
    "                  \"C\":[7,np.nan,9,10,11]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42d242-7262-4dc8-a27c-b32fc187a404",
   "metadata": {},
   "source": [
    "1) Deletion: This technique involves removing the observations or variables that contain missing values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6c9679d-19fd-4dd7-a634-5f556dd7c0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C\n",
       "0  1.0  3.0   7.0\n",
       "1  2.0  4.0   NaN\n",
       "2  3.0  5.0   9.0\n",
       "3  NaN  NaN  10.0\n",
       "4  4.0  NaN  11.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7405b4-da9d-4017-9309-742d767ddba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "319d0081-c65b-49fa-90a4-6c28d4274c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C\n",
       "0  1.0  3.0  7.0\n",
       "2  3.0  5.0  9.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f41f02-df96-4718-8f31-360b83b14e9f",
   "metadata": {},
   "source": [
    "2) Imputation: This technique involves filling in the missing values with a predicted or estimated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a2ca6a-4845-4077-a9a5-3724258d1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"A\":[1,2,3,np.nan,4],\n",
    "                  \"B\":[3,4,5,np.nan,np.nan],\n",
    "                  \"C\":[7,np.nan,9,10,11]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c638967-44c4-4fc7-9281-b64c6433f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78389181-8895-4cff-83d1-91d395697509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B      C\n",
       "0  1.0  3.0   7.00\n",
       "1  2.0  4.0   9.25\n",
       "2  3.0  5.0   9.00\n",
       "3  2.5  4.0  10.00\n",
       "4  4.0  4.0  11.00"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95dfd6c-cf76-4de2-84ba-3b2be3bad7d3",
   "metadata": {},
   "source": [
    "### Ans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422d8d1-8f2e-4cb9-84a3-e8c3e6b73e00",
   "metadata": {},
   "source": [
    "Imbalanced data is a situation where the proportion of observations in different classes or categories in a dataset is not equal. In other words, one class may have significantly more or fewer observations than the other classes. This often occurs in real-world datasets, such as medical diagnosis, fraud detection, and spam filtering, where the rare events or outcomes are of more interest.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased and inaccurate machine learning models. In particular, if the algorithm is trained on imbalanced data, it may prioritize the majority class and ignore the minority class. As a result, the model may have low accuracy, low precision, and high recall for the majority class, but low recall and high false negative rate for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fade4-e604-4342-8df7-3ec849400eba",
   "metadata": {},
   "source": [
    "### Ans4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0394e-be2b-495e-820e-8a4ff04ba6ff",
   "metadata": {},
   "source": [
    "1) Upsampling involves increasing the number of instances in the minority class by either duplicating the existing observations or generating synthetic observations using techniques such as SMOTE (Synthetic Minority Over-sampling Technique). The goal of upsampling is to provide more examples of the minority class to the machine learning algorithm and help it learn the patterns and characteristics of the minority class more effectively.\n",
    "\n",
    "2) Downsampling involves decreasing the number of instances in the majority class by randomly removing some observations from the dataset. The goal of downsampling is to reduce the bias towards the majority class and provide a more balanced representation of the different classes to the machine learning algorithm.\n",
    "\n",
    "Here is an example to illustrate when upsampling and downsampling may be required:\n",
    "\n",
    "\n",
    "Suppose we have a dataset of credit card transactions with 100,000 observations, out of which only 1,000 are fraudulent transactions. In this case, the dataset is highly imbalanced, with the fraudulent transactions representing only 1% of the total observations.\n",
    "\n",
    "If we train a machine learning model on this imbalanced dataset without any data balancing techniques, the model may not be able to detect the fraudulent transactions effectively and may have a high false negative rate. To address this problem, we can use upsampling or downsampling.\n",
    "\n",
    "If we choose to upsample the minority class, we can increase the number of fraudulent transactions from 1,000 to 10,000 by either duplicating the existing observations or generating synthetic observations. This will provide more examples of the fraudulent transactions to the machine learning algorithm and help it learn the patterns and characteristics of the fraudulent transactions more effectively.\n",
    "\n",
    "On the other hand, if we choose to downsample the majority class, we can randomly remove some of the non-fraudulent transactions from the dataset, say 80,000, to reduce the bias towards the majority class. This will provide a more balanced representation of the different classes to the machine learning algorithm and help it learn the patterns and characteristics of both classes more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8449574-e4f3-4639-b96a-a3911698df2d",
   "metadata": {},
   "source": [
    "### Ans5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1e5c4-933c-46dc-9632-9c6e8de7bc07",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new samples that are similar to the existing ones, but have small variations. The goal of data augmentation is to improve the generalization ability of machine learning models by providing them with more diverse and representative training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b960e-5d1c-4cf8-b66e-66fa61dcdabe",
   "metadata": {},
   "source": [
    "One popular technique for data augmentation is SMOTE (Synthetic Minority Over-sampling Technique), which is specifically designed to address the problem of imbalanced data. SMOTE works by generating synthetic samples of the minority class by interpolating between existing samples.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "Select a minority sample x.\n",
    "\n",
    "Choose k nearest neighbors of x from the minority class.\n",
    "\n",
    "Randomly choose one of the k nearest neighbors, say x'.\n",
    "\n",
    "Generate a synthetic sample by linearly interpolating between x and x':\n",
    "\n",
    "x_new = x + (x' - x) * r, where r is a random number between 0 and 1.\n",
    "\n",
    "Repeat steps 1-4 until the desired number of synthetic samples is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b18ac0-2207-4c7e-b8bf-26f77f1d8379",
   "metadata": {},
   "source": [
    "### Ans6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60051754-7e80-423c-97b8-a110cc198d34",
   "metadata": {},
   "source": [
    "Outliers are data points in a dataset that are significantly different from other data points. These data points are usually located far away from the rest of the data points, and they can have a significant impact on the results of data analysis and machine learning models.\n",
    "\n",
    "It is essential to handle outliers for several reasons:\n",
    "\n",
    "1) Outliers can affect the statistical measures of a dataset, such as the mean and standard deviation, making them less representative of the actual data distribution.\n",
    "\n",
    "2) Outliers can affect the accuracy of predictive models by introducing bias and reducing the model's ability to generalize to new data.\n",
    "\n",
    "3) Outliers can also affect the performance of clustering algorithms by creating clusters that are not representative of the underlying data distribution.\n",
    "\n",
    "4) Outliers can also be caused by errors in data collection or data entry, and their presence can signal problems in the data collection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525aed3-587c-4327-a1cb-0201a021db67",
   "metadata": {},
   "source": [
    "### Ans7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7d142-4ac4-4891-9d73-40d61d2fa5d0",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis. Here are some of the most commonly used techniques:\n",
    "\n",
    "1) Delete the missing data: One technique is to delete the missing data from the dataset. This is only appropriate if the amount of missing data is small and the remaining data is still representative of the underlying population.\n",
    "\n",
    "2) Impute missing data: Another technique is to impute the missing data by replacing the missing values with an estimate of the missing value. This can be done using techniques such as mean imputation, median imputation, mode imputation, or regression imputation.\n",
    "\n",
    "3) Use data augmentation techniques: Data augmentation techniques such as synthetic minority oversampling technique (SMOTE) can be used to generate synthetic data that can be used to replace the missing values.\n",
    "\n",
    "4) Use machine learning algorithms: Machine learning algorithms such as k-Nearest Neighbors (KNN) and Decision Trees can be used to predict missing values based on the other variables in the dataset.\n",
    "\n",
    "5) Use expert knowledge: Expert knowledge can be used to estimate missing values in a dataset based on the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63604156-a34d-4f3b-bd79-9934e6a49c4a",
   "metadata": {},
   "source": [
    "### Ans8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7af12-6fad-434f-9ac3-7f75381fa5ac",
   "metadata": {},
   "source": [
    "Here are some strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "Missing data analysis: Conduct a missing data analysis to identify any patterns or correlations between the missing data and the other variables in the dataset. This can involve visualizing the missing data patterns and conducting statistical tests such as chi-square tests or t-tests.\n",
    "\n",
    "Correlation analysis: Conduct a correlation analysis to identify any correlations between the missing data and the other variables in the dataset. This can involve calculating correlation coefficients between the missing data and other variables in the dataset.\n",
    "\n",
    "Imputation tests: Conduct imputation tests by imputing the missing data using different imputation techniques and comparing the results. If the imputed values are significantly different from the observed values, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "Expert knowledge: Use expert knowledge to identify any potential reasons for the missing data and whether there is a pattern to the missing data.\n",
    "\n",
    "Machine learning algorithms: Use machine learning algorithms such as random forests or logistic regression to predict the missing values based on the other variables in the dataset. If the prediction accuracy is high, it may indicate that the missing data is missing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c68d0a-0d79-4e00-8b42-c12b364b60ba",
   "metadata": {},
   "source": [
    "### Ans9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ead81-1de1-4a9f-9f51-67f44e5628f9",
   "metadata": {},
   "source": [
    "Here are some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "1) Confusion Matrix: Confusion matrix can be used to evaluate the performance of a machine learning model on an imbalanced dataset. It provides information about true positives, true negatives, false positives, and false negatives, which can be used to calculate metrics such as precision, recall, F1-score, and accuracy.\n",
    "\n",
    "2) Precision-Recall Curve: A precision-recall curve can be plotted to evaluate the performance of the model on an imbalanced dataset. It shows the trade-off between precision and recall for different classification thresholds, and the area under the curve (AUC) can be used as a performance metric.\n",
    "\n",
    "3) ROC Curve: ROC (Receiver Operating Characteristic) curve can also be used to evaluate the performance of the model. It shows the trade-off between true positive rate (sensitivity) and false positive rate (1 - specificity) for different classification thresholds, and the area under the curve (AUC) can be used as a performance metric.\n",
    "\n",
    "4) Stratified Sampling: When splitting the dataset into training and testing sets, it is important to use stratified sampling to ensure that the proportion of positive and negative examples is maintained in both sets. This can help to prevent overfitting and ensure that the model is not biased towards the majority class.\n",
    "\n",
    "5) Resampling Techniques: Resampling techniques such as oversampling the minority class or undersampling the majority class can be used to balance the class distribution in the dataset. This can help to improve the performance of the model on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7795536-1145-4705-bf3c-5adbea4f41a5",
   "metadata": {},
   "source": [
    "### Ans10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868748c-472a-46c8-9a15-3603636f01f7",
   "metadata": {},
   "source": [
    "Here are some methods to balance the dataset and down-sample the majority class:\n",
    "\n",
    "1) Undersampling: Undersampling involves randomly selecting a subset of the majority class samples to match the number of minority class samples. This can help to balance the class distribution in the dataset.\n",
    "\n",
    "2) Oversampling: Oversampling involves generating synthetic samples for the minority class to match the number of majority class samples. This can be done using techniques such as SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN (Adaptive Synthetic Sampling).\n",
    "\n",
    "3) Hybrid Sampling: Hybrid sampling involves combining both undersampling and oversampling techniques to balance the dataset. This can be done using techniques such as SMOTE combined with Tomek links, which involves applying SMOTE and then removing any overlapping samples with the majority class using Tomek links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de661f6-63f2-4931-844d-f8179daf2825",
   "metadata": {},
   "source": [
    "### Ans11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8f27d-eb15-4191-98fa-be0ef1ff53bc",
   "metadata": {},
   "source": [
    "Here are some methods that can be used to up-sample the minority class:\n",
    "\n",
    "1) Random oversampling: In this method, we randomly duplicate samples from the minority class to increase its size to match the size of the majority class. This method is simple and fast, but it can lead to overfitting and reduce the diversity of the data.\n",
    "\n",
    "2) Synthetic Minority Over-sampling Technique (SMOTE): In this method, we create synthetic samples of the minority class by interpolating between pairs of samples from the minority class. This method can increase the diversity of the data and prevent overfitting, but it can also generate noisy samples.\n",
    "\n",
    "3) Adaptive Synthetic (ADASYN): This method is an extension of SMOTE, which creates more synthetic samples in regions where the density of the minority class is lower. This method can further increase the diversity of the data and address the issue of generating noisy samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6f0a7-c0fc-4354-a8ae-6f9d9bc1bf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
